\section{Introduction to Proof Theory}
\subauthor{Sonia Marin}{University of Birmingham}
\subauthor{Marianna Girlando}{University of Amsterdam}
\plquote{``Some people should not talk. And you're not one of these people.''}

\subsection{Introduction}
Proof theory is the study of \emph{reasoning} - mathematical or otherwise.
The first foundations of proof theory were laid in the 19\textsuperscript{th}
century in order to formalize the concept of ``proofs'' and ``evidence''.
In the early 20\textsuperscript{th} century,, Russell and Whitehead's
Principia Mathematica laid formal foundations for mathematics. At around the same 
time, Hilbert wanted a formal and consistent framework for mathematics.
He wanted to be able to decide for each theorem in this framework whether
it is valid or not. Unfortunately, in 1930 G\"odel published his
Incompleteness Theorems that prove that every sufficiently powerful and consistent
logical system cannot be complete. Later that decade, Church proved the
undecidability of classical predicate logic.

This course will discuss classical logic and arithmetic, and then go on to discuss
intuitionistic/constructive logic and its relation to PL theory. Then we will
present natural deduction and sequent calculus as well as cut elimination
and normalization. Lastly, we will return to arithmetic with a new viewpoint.

\subsection{Syntax of Propositional and Predicate Logic}
Propositional logic involves countably many propositional variables: $\mathrm{Var} = {p_1, ...}$. 
Propositional variables can be either $0$ or $1$. We define the connectives $\wedge, \vee, \to$ 
and the constant $\bottom$.

The set of formulas $\mathrm{Form}$ is the smallest set containing the propositional variables and 
the constant, such that if $A, B$ are formulas, so are $A \vee B, A \wedge B, A \to B$.

A propositional assignment assigns values to variables: $\varphi: \mathrm{Var} \to \{0, 1\}$.
We extend the definition to formulas by induction on the structure of the formula.

This logic is fairly simple and not complicated. We shall define a more complex logic -
the predicate (first-order) logic.
The predicate language $\mathcal{L}^=$ will have:
\begin{itemize} 
\item Countably many variables,
\item A set of function symbols, each with some fixed arity,
\item A set of predicate symbols, each with some fixed arity,
\item The equality symbol $=$ which is a $2$-ary predicate,
\item The propositional constant $\bottom$,
\item The connectives $\vee, \wedge, \to$
\end{itemize}

Terms are defined inductively: Variables are terms, and an instantiation of a function with the right
number of terms is also a term. Atomic formulas are also defined inductively: If $s, t$ are terms
then $s = t$ is an atomic formula, and an instantiation of a predicate with the right number of terms
is an atomic formula.

Formulas are, too, defined inductively: Atomic formulas are formulas, as is $\bottom$. If
$A, B$ are formulas then so are $A\vee B, A \wedge B, A \to B$.
Also, if $A$ is a formula, then $\exists x. A, \forall x. A$ are formulas.

\subsection{Proof Systems}
A proof system consists of a set of axioms and a set of inference rules. A proof of a formula
is then constructed by chaining together axioms, inference rules and objects
generated by axioms and inference rules, until $A$ is reached. Today we will look at
Hilbert-Frege proof systems, also called axiom systems. We will see an axiom system
for propositional logic and an axiom system for first-order logic. Lastly, we will
review first-order theories and Peano arithmetic.

We will now see an axiom system for classical propositional logic. These are the axioms:
\[
\begin{array}{cc}
\text{PL1}& A \to (B \to A)\\
\text{PL2}& (A \to (B \to C)) \to ((A \to B) \to (A \to C))\\
\text{PL3}& (A \wedge A) \to A\\
\text{PL4}& (A \wedge B) \to B\\
\text{PL5}& A \to (B \to (A \wedge B))\\
\text{PL6}& A \to (A \vee B)\\
\text{PL7}& B \to (A \vee B)\\
\text{PL8}& (A \to C) \to ((B \to C) \to ((A \vee B) \to C))\\
\text{PL9}& \bottom \to A\\
\text{PL10}& A \vee (A \to \bottom)\\
\end{array}
\]

In addition, we have the following inference rule: $\frac{A \to B~~~~A}{B}$ which is to be called 
\emph{modus ponens} or MP.
A derivation of $A$ from assumptions $\Gamma$ is a list of formulas such that $A_n = n$
and for each $i \leq n$, either $A_i$ is an axiom, $A_i \in \Gamma$ or $A_i$ can be obtained
by applying MP to formulas in $A_1, ..., A_{i - 1}$. We write $\Gamma \vdash A$ if there
is a derivation of $A$ from formuals in $\Gamma$. A proof of $A$ is a derivation
of $A$ from $\emptyset$. We write $\vdash A$ if there is a proof of $A$.
Classical propositional logic is the set of formulas with a proof.

\textbf{Theorem. (Deduction)} For a formula $A$ from the propositional logic, and $\Gamma$
which is a set of propositional logic formulas, $\Gamma \vdash A \to B$ if and only if 
$\Gamma \cup {A} \vdash B$.

\subsection{First-Order Logic}
Now, we will see another axiom system, one for first-order logic.

Let $A, B, C$ be first-order logic formulas. We will take all axioms and inference
rules from the propositional logic system, and also:
\[
\begin{array}{ccc}
\text{FO1}&\forall x. (A(x)) \to A(t)& \\
\text{FO2}&A(t) \to \exists x. (A(x))& \\
\text{FO3}&\forall x. (A \to B(x)) \to (A \to \forall x. B(x))& \text{if} x \notin \mathrm{FV}(A)\\
\text{FO4}&\forall x. (A(x) \to B) \to (\exists (A(x)) \to B)& \text{if} x \notin \mathrm{FV}(B) \\
\text{FO5}&\forall x. x = x& \\
\text{FO6}&\forall x. \forall y. (x = y \to (A(x) \to A(y)))& 
\end{array}
\]

In addition, we have the following inference rule: $\frac{A}{\forall x. A}$ which is to be called \emph{generalization}
or gen.

Proofs are defined similarly to how they are defined for propositional logic, except we use the extended
axiom set and we can also use gen as well as MP.

We will now define a \emph{first-order theory} $\mathcal{T}$: it consists of a predicate 
language $\mathcal{L}_\mathcal{T}^=$ as well as a set of formulas in the language $\mathcal{L}_\mathcal{T}^=$
which are the non-logical axioms of the theory.

An example for a first-order theory is the one with the predicate language $\mathcal{L}_\mathrm{PA}^= = \{0, s, +, \cdot\}$.
$0$ is a constant. $s$ is a unary function symbol. $+, \cdot$ are binary function symbols. 
We will have the following non-logical axioms:

\[
\begin{array}{ccc}
\text{PA1}& \forall x (0 \neq s(x))& \\
\text{PA2}& \forall x. \forall y. (s(x) = s(y) \to x = y)& \\
\text{PA3}& \forall x.(x + 0 = x)& \\
\text{PA4}& \forall x. \forall y. (x + s(y) = s(x+y))&\\
\text{PA5}& \forall x. (x \cdot 0 = 0)&\\
\text{PA6}& \forall x. \forall y. (x \cdot s(y) = (x \cdot y) + x) &\\
\text{PA7}& (A(0) \wedge (\forall A(x) \to A(s(x)))) \to \forall x. A(x)& \text{where~} A \in \mathcal{L}_\mathrm{PA}^=, x\in \mathrm{FV}(A)
\end{array}
\]

This is the \emph{Peano arithmetic}.

\subsection{Intuitionistic Logic}
Intuitionistic mathematics - an alternative solution for the problem of finding a foundation for mathematics.
Intuitionistic logic rejects the fact that there are 2 truth values. This is an important feature
of many logic systems, allowing, for example, proofs by contradiction and the usage of the law of excluded middle.

The syntax of intuitionistic propositional logic is similar to that of classical propositional logic:
we have a variable set $\mathrm{Var}$ and the connectives $\bottom, \wedge, \vee, \to$.

The axioms are the same as those in CPL, except the last one is missing (excluded middle). MP is still there. 
So, the axioms are:
\[
\begin{array}{cc}
\text{IPL1}& A \to (B \to A)\\
\text{IPL2}& (A \to (B \to C)) \to ((A \to B) \to (A \to C))\\
\text{IPL3}& (A \wedge A) \to A\\
\text{IPL4}& (A \wedge B) \to B\\
\text{IPL5}& A \to (B \to (A \wedge B))\\
\text{IPL6}& A \to (A \vee B)\\
\text{IPL7}& B \to (A \vee B)\\
\text{IPL8}& (A \to C) \to ((B \to C) \to ((A \vee B) \to C))\\
\text{IPL9}& \bottom \to A\\
\end{array}
\]

Non-provable formulas:
\[
\begin{array}{cc}
A \vee \neg A  & \text{(Excluded middle)} \\
\neg \neg A \to A & \text{(Double negation)} \\
((A \to B) \to A) \to A & \text{(Peirce's law)} \\
(A \to B) \to (\neg A \vee B) & 
\end{array}
\]

Provable formulas:
\[
\begin{array}{c}
(\neg A \vee B) \to (\neg A \vee B) \\
(A \vee B) \to \neg (\neg A \wedge \neg B)
\end{array}
\]

\subsection{Natural Deduction}
Axioms have the advantage of making the interactions between connectives and the principles
of mathematical reasoning precise. However, searching for a proof using MP is painful.

Natural deduction has introduction rules and elimination rules. Introduction rules give
 a sufficient condition for deriving a statement depending on its form.
For example, an introduction rule for $A \wedge B$ may rely on having proved $A, B$.
Additional introduction rules include $\frac{A}{A\vee B}$, $\frac{B}{A\vee B}$,
$\frac{B}{A\to B}$.

There are also elimination rules allowing the derivation of simpler syntactic forms
from more complicated ones. For example, MP, or $\frac{A\wedge B}{A}$.
Additional rules include $\frac{A\vee B~~~~A\to C~~~~B\to C}{C}$ and $\frac{\bottom}{A}$.

A derivation of a formula $A$ from a set of assumptions is a
tree in which each formula is either an assumption from $\Gamma$ or
the conclusion of a correct application of an inference rule. If every assumption
is \emph{discharged}, it is a \emph{proof} of $A$.

This system is also called NJ\footnote{Stands for natural deduction. \emph{(sic)}}. NJ is a complete system
in the sense that it can prove any statement that can be proven in the Hilbert proof system
for the propositional logic. It is also sound in the sense that every statement that it proves,
can also be proven in the Hilbert proof system.

From a proof theory point of view, the Curry-Howard Correspondence is an
interpretation of the fact that formulas
must be proven and not just be true. The CHC is the correspondence between
types and formulas, terms and proofs and reductions and normalization.

Proofs can be seen as $\lambda$-terms. 
For example, conjunction introduction is like a product operator and conjunction elimination is like projection.
Disjunction introduction is like a case statement and disjunction elimination is like disjoint sum elimination
(case analysis).

\subsection{Sequent Calculus}
The Sequent Calculus works by instead of starting from assumptions and working down, we start from
what we want to prove and go back using rules.

A sequent is a judgement of the form $\Gamma \To A$. $\Gamma$ is a multiset of formulas and $A$ is a formula.
We think of this judgement as meaning $\bigwedge \Gamma \to A$. The following are the rules of the
sequent calculus:

% \[
% \begin{array}{rl}
\begin{alignat*}{2}
\setstretch{2}
\text{init}&\frac{}{p, \Gamma \To p} \\
\bottom&\frac{}{\bottom, \Gamma \To C}\\
\wedge_L&\frac{A, B, \Gamma \To C}{A\wedge B, \Gamma \To C}\\
\wedge_R&\frac{\Gamma \To A~~~~\Gamma\To B}{\Gamma \To A \wedge B}\\
\vee_L&\frac{A, \Gamma \To C~~~~B, \Gamma \To C}{A\vee B, \Gamma \To C}\\
\vee_R, i\in \{0,1\}&\frac{\Gamma \To A_i}{\Gamma \to A_0 \vee A_1}\\
\to_L&\frac{A\to B, \Gamma \To A~~~~B, \Gamma \To C}{A\to B, \Gamma \To C}\\
\to_R&\frac{A, \Gamma \To B}{\Gamma \To A \to B}\\
\text{cut}&\frac{\Gamma \To A~~~~A, \Gamma \To C}{\Gamma \To C}
\end{alignat*}
% \end{array}
% \]

A derivation in LJ is a rooted tree whose nodes are labeled by sequents such that
the leaves of the tree are the initial sequent, the segments occupying
intermediate nodes in the tree are obtained from the sequents occupying
 the nodes directly above them by means of a correct application of inference rules,
 and the root of the tree is the conclusion of the derivation.

We denote by $\vdash_\mathrm{LJ} \Gamma \To C$ if there is a derivation of $\Gamma \to C$ in NJ.

\subsubsection{Cut Elimination}

\textbf{Theorem. (Gentzen)} Every theorem in LJ has a proof that does not use the cut rule.

\textbf{Corollary. (Analyticity)} Every theorem of LJ has a proof that contains only
subformulas of the theorem.

The idea of the proof is pushing the cut up to to apply them on smaller and smaller formulas,
until the cuts disappear. Following is a sketch of the proof, which is quite complex.
\begin{itemize}
\item \textbf{Notations and definitions:} We define the \emph{height} of a derivation $D$, 
$\mathrm{ht}(D)$, to be the length of its longest branch minus 1. We define the \emph{level} of a 
cut rule to be the sum of the heights of derivations of the two premises of cut. The \emph{degree} of 
a formula is the number of logical connectives in the formula, The \emph{rank} of a cut rule is the degree
of the cut formula $A$ plus 1. The rank of the derivation $D$ is the maximum rank of
all cut formulas occuring in $D$. $\Gamma \To_p^m C$ means that there is a derivation of $\Gamma \To C$
of height at most $m$ and rank at most $p$.
\item \textbf{Lemma: Closure under weakening.} If $\Gamma \To_p^m C$m then for all $\Gamma'$, $\Gamma', \Gamma \To_p^m C$.
\item \textbf{Lemma: Closure under contraction.} If $A, A, \Gamma \To_p^m C$, then $A, \Gamma \To_p^m C$.
\item \textbf{Lemma: Closure under cut.} If $\Gamma \To_0^m A$ and $A, \Gamma \To_0^n C$, then $\Gamma \To$ \todo{missed it.}
\item \textbf{Theorem: Cut Elimination.} If we have a derivation $D$ of $\Gamma \To_p C$, we
can construct a derivation $D^*$ of $\Gamma \To_0 C$.
\end{itemize}

The construction takes $4 \uparrow \uparrow n$ steps to remove $n$ cuts.
This theorem gives us the important property of \emph{analyticity} described earlier. We do need cuts
for the simulation of MP (for completeness with respect to the Hilbert system).

\subsubsection{Normalization}
Proof normalization concerns the removal of derivation subsequences comprising an introduction rule
and directly following it an elimination rule whose premise is the conclusion of the introduction rule.
These are called \emph{detours}. Elimination of detours in NJ derivations corresponds
to normalization of $\lambda$-terms, such as $\beta$-reductions which correspond to MP.

By normalization, we can prove that if $\Gamma \vdash_\mathrm{NJ} A$, then $\Gamma \vdash_\mathrm{LJ} \Gamma \To A$.
The other direction holds as well: If $\Gamma \vdash_\mathrm{LJ} \Gamma \To A$, then $\Gamma \vdash_\mathrm{LJ} A$. 
Normalized NJ formulas correspond to cut-free LJ formulas.

As for the Curry-Howard correspondence, only a subset of $\mathrm{NJ}$ typable $\lambda$-terms
are typable in $\mathrm{LJ}$\footnote{This subset includes all normalized formulas.}. 

\subsubsection{Sequent Calculus for Classical First-Order Logic}
It is possible to write a sequent calculus for CFOL:
\[
\begin{array}{cc}
\forall_L& \frac{A(t), \forall x. A(x), \Gamma \vdash \Delta}{\forall x. A(x), \Gamma \vdash \Delta} \\
\forall_R& \frac{\Gamma \vdash \Delta, A(y)}{\Gamma \vdash \Delta, \forall x. A(x)} \\
\exists_L& \frac{A(y), \Gamma \vdash \Delta}{\exists x. A(x), \Gamma \vdash \Delta} \\
\exists_R& \frac{\Delta \vdash \Delta, A(t), \exists x. A}{\Gamma \vdash \Delta. \exists x. A(x)} \\
=_1& \frac{x = y, \Delta \vdash \Delta, A(x), A(y)}{x = y, \Gamma \vdash \Delta, A(x)} \\
=_2& \frac{x = y, \Delta \vdash \Delta, A(x), A(y)}{x = y, \Gamma \vdash \Delta, A(y)}
\end{array}
\]

\subsubsection{Sequent Calculus for Peano Arithmetic}
Also, it is possible to write a sequent calculus for PA: we shall take the sequent calculus
for CFOL and add 6 initial sequents for axioms PA1 - PA6, and also an induction rule:
\[
\begin{array}{cc}
\text{ind}&\frac{\Gamma \To \Delta, A(0)~~~~A(x), \Gamma \To \Delta, A(s(x))}{\Gamma \To \Delta, \forall x. A(x)}
\end{array}
\]

